"""
Fokker-Planck equation implementations for financial models.

This module implements the Fokker-Planck equation (FPE) governing the evolution
of probability density functions for stochastic processes.
"""

from typing import Optional
import torch
import numpy as np

from .base import BasePDE, BaseSDE


class FokkerPlanckMertonND(BasePDE, BaseSDE):
    """
    N-dimensional Fokker-Planck equation for multivariate Merton model.

    The n-dimensional Merton model assumes:
        dX_t = Î¼ dt + Î£ dW_t

    where:
        X_t âˆˆ â„â¿: state vector
        Î¼ âˆˆ â„â¿: constant drift vector
        Î£ âˆˆ â„â¿Ë£â¿: constant volatility matrix
        W_t âˆˆ â„â¿: n-dimensional Brownian motion

    The probability density p(x, t) satisfies the Fokker-Planck equation:
        âˆ‚p/âˆ‚t + Î¼Â·âˆ‡p - (1/2)âˆ‘áµ¢â±¼ Dáµ¢â±¼ âˆ‚Â²p/âˆ‚xáµ¢âˆ‚xâ±¼ = 0

    where D = Î£Î£áµ€ is the diffusion matrix.

    For constant coefficients starting from xâ‚€, the analytical solution is:
        p(x, t) = ð’©(x; xâ‚€ + Î¼t, Dt)

    Args:
        spatial_dim: Dimension of state space (n)
        mu: Drift vector [n] or scalar (broadcasted to all dimensions)
        sigma: Volatility matrix [n, n] or vector [n] (diagonal) or scalar (isotropic)
        x0: Initial state [n] or scalar (broadcasted)
        device: Computation device
        dtype: Tensor data type

    Example:
        >>> # 2D model with diagonal covariance
        >>> equation = FokkerPlanckMertonND(
        ...     spatial_dim=2,
        ...     mu=[0.1, 0.05],
        ...     sigma=[0.2, 0.3],  # diagonal: Ïƒâ‚=0.2, Ïƒâ‚‚=0.3
        ...     x0=[0.0, 0.0]
        ... )
    """

    def __init__(
        self,
        spatial_dim: int,
        mu: Optional[float | list | np.ndarray] = None,
        sigma: Optional[float | list | np.ndarray] = None,
        x0: Optional[float | list | np.ndarray] = None,
        device: torch.device = torch.device('cpu'),
        dtype: torch.dtype = torch.float32
    ):
        # Explicitly call BasePDE.__init__ to set shared attributes (spatial_dim, device, dtype).
        # BaseSDE has an identical __init__ signature; calling BasePDE is sufficient since
        # both set the same attributes.
        BasePDE.__init__(self, spatial_dim=spatial_dim, device=device, dtype=dtype)

        # Process drift vector Î¼
        if mu is None:
            mu = 0.0
        if np.isscalar(mu):
            mu = np.full(spatial_dim, mu)
        else:
            mu = np.asarray(mu)
            if mu.shape != (spatial_dim,):
                raise ValueError(f"mu must have shape ({spatial_dim},), got {mu.shape}")

        self.mu = torch.tensor(mu, dtype=dtype, device=device)

        # Process volatility matrix Î£
        if sigma is None:
            sigma = 1.0
        if np.isscalar(sigma):
            # Isotropic: Î£ = ÏƒI
            sigma = np.eye(spatial_dim) * sigma
        else:
            sigma = np.asarray(sigma)
            if sigma.ndim == 1:
                # Diagonal: Î£ = diag(Ïƒ)
                if sigma.shape != (spatial_dim,):
                    raise ValueError(f"sigma vector must have shape ({spatial_dim},), got {sigma.shape}")
                sigma = np.diag(sigma)
            elif sigma.ndim == 2:
                # Full matrix
                if sigma.shape != (spatial_dim, spatial_dim):
                    raise ValueError(
                        f"sigma matrix must have shape ({spatial_dim}, {spatial_dim}), got {sigma.shape}"
                    )
            else:
                raise ValueError(f"sigma must be scalar, vector, or matrix, got shape {sigma.shape}")

        self.sigma = torch.tensor(sigma, dtype=dtype, device=device)

        # Compute diffusion matrix D = Î£Î£áµ€
        self.D = torch.matmul(self.sigma, self.sigma.T)

        # Process initial state xâ‚€
        if x0 is None:
            x0 = 0.0
        if np.isscalar(x0):
            x0 = np.full(spatial_dim, x0)
        else:
            x0 = np.asarray(x0)
            if x0.shape != (spatial_dim,):
                raise ValueError(f"x0 must have shape ({spatial_dim},), got {x0.shape}")

        self.x0 = torch.tensor(x0, dtype=dtype, device=device)

    def pde_residual(
        self,
        x: torch.Tensor,
        t: torch.Tensor,
        u: torch.Tensor,
        u_t: torch.Tensor,
        u_x: torch.Tensor,
        u_xx: torch.Tensor
    ) -> torch.Tensor:
        """
        Compute n-dimensional Fokker-Planck PDE residual.

        The n-D FPE:
            âˆ‚p/âˆ‚t + Î¼Â·âˆ‡p - (1/2)âˆ‘áµ¢â±¼ Dáµ¢â±¼ âˆ‚Â²p/âˆ‚xáµ¢âˆ‚xâ±¼ = 0

        Args:
            x: Spatial coordinates [Batch, spatial_dim]
            t: Time coordinates [Batch, 1]
            u: Density p(x, t) [Batch, 1]
            u_t: Time derivative âˆ‚p/âˆ‚t [Batch, 1]
            u_x: Spatial gradient âˆ‡p [Batch, spatial_dim]
            u_xx: Hessian matrix [Batch, spatial_dim, spatial_dim]

        Returns:
            PDE residual [Batch, 1]
        """
        # Drift term: Î¼Â·âˆ‡p
        # u_x shape: [Batch, spatial_dim]
        # mu shape: [spatial_dim]
        drift_term = torch.sum(self.mu * u_x, dim=-1, keepdim=True)  # [Batch, 1]

        # Diffusion term: (1/2) âˆ‘áµ¢â±¼ Dáµ¢â±¼ âˆ‚Â²p/âˆ‚xáµ¢âˆ‚xâ±¼
        # u_xx shape: [Batch, spatial_dim, spatial_dim]
        # D shape: [spatial_dim, spatial_dim]
        # Contract: âˆ‘áµ¢â±¼ Dáµ¢â±¼ Háµ¢â±¼ = Tr(D @ H^T) = Tr(D @ H) (since H is symmetric)
        diffusion_term = torch.einsum('ij,bij->b', self.D, u_xx).unsqueeze(-1)  # [Batch, 1]
        diffusion_term = 0.5 * diffusion_term

        # Residual: âˆ‚p/âˆ‚t + Î¼Â·âˆ‡p - (1/2)âˆ‘áµ¢â±¼ Dáµ¢â±¼ âˆ‚Â²p/âˆ‚xáµ¢âˆ‚xâ±¼
        residual = u_t + drift_term - diffusion_term

        return residual

    def initial_condition(self, x: torch.Tensor) -> torch.Tensor:
        """
        Initial condition: Dirac delta at xâ‚€.

        For numerical stability, we approximate Î´(x - xâ‚€) with a narrow Gaussian:
            p(x, 0) â‰ˆ ð’©(x; xâ‚€, ÎµÂ²I)

        Args:
            x: Spatial coordinates [Batch, spatial_dim]

        Returns:
            Initial density [Batch, 1]
        """
        epsilon = 0.01  # Regularization parameter
        variance = epsilon ** 2

        # Multivariate Gaussian
        diff = x - self.x0  # [Batch, spatial_dim]
        exponent = -0.5 * torch.sum(diff ** 2, dim=-1, keepdim=True) / variance  # [Batch, 1]
        coeff = 1.0 / ((2 * np.pi * variance) ** (self.spatial_dim / 2))
        p0 = coeff * torch.exp(exponent)

        return p0

    def analytical_solution(
        self,
        x: torch.Tensor,
        t: torch.Tensor
    ) -> torch.Tensor:
        """
        Analytical solution: multivariate Gaussian.

        For constant Î¼ and D, starting from xâ‚€, the solution is:
            p(x, t) = ð’©(x; xâ‚€ + Î¼t, Dt)

        Args:
            x: Spatial coordinates [Batch, spatial_dim]
            t: Time values [Batch, 1] or [Batch]

        Returns:
            Analytical density [Batch, 1]
        """
        # Ensure proper shapes
        if x.dim() == 1:
            x = x.unsqueeze(0)  # [1, spatial_dim]
        if t.dim() == 1:
            t = t.unsqueeze(-1)  # [Batch, 1]

        # Avoid t=0
        t_clamped = torch.clamp(t, min=1e-6)

        batch_size = x.shape[0]

        # Time-evolved mean and covariance (per-sample)
        mu_t = self.x0 + self.mu * t_clamped  # [Batch, spatial_dim]

        # Compute per-sample covariance: cov_t[b] = D * t[b]
        # Reshape t for proper broadcasting: [Batch, 1] -> [Batch, 1, 1]
        t_expanded = t_clamped.view(batch_size, 1, 1)  # [Batch, 1, 1]
        # Expand D to batch dimension: [spatial_dim, spatial_dim] -> [Batch, spatial_dim, spatial_dim]
        cov_t = self.D.unsqueeze(0) * t_expanded  # [Batch, spatial_dim, spatial_dim]

        # Multivariate Gaussian PDF
        # p(x) = (2Ï€)^(-n/2) |Î£|^(-1/2) exp(-1/2 (x-Î¼)áµ€ Î£â»Â¹ (x-Î¼))

        diff = x - mu_t  # [Batch, spatial_dim]

        # Compute determinant and inverse of covariance matrix (batched)
        det_cov = torch.linalg.det(cov_t)  # [Batch]
        inv_cov = torch.linalg.inv(cov_t)  # [Batch, spatial_dim, spatial_dim]

        # Mahalanobis distance: (x-Î¼)áµ€ Î£â»Â¹ (x-Î¼) (batched)
        mahal = torch.einsum('bi,bij,bj->b', diff, inv_cov, diff)  # [Batch]

        # Gaussian PDF
        coeff = 1.0 / torch.sqrt((2 * np.pi) ** self.spatial_dim * det_cov)
        p = coeff * torch.exp(-0.5 * mahal)

        return p.unsqueeze(-1)  # [Batch, 1]

    def drift(self, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:
        """
        Compute drift coefficient f(x, t) = Î¼ (constant drift).

        Args:
            x: State coordinates [Batch, spatial_dim]
            t: Time values [Batch, 1]

        Returns:
            Drift vector Î¼ [Batch, spatial_dim]
        """
        batch_size = x.shape[0]
        # Expand constant drift to batch size
        return self.mu.unsqueeze(0).expand(batch_size, -1)

    def diffusion(self, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:
        """
        Compute diffusion coefficient G(x, t) = Î£ (constant diffusion).

        Args:
            x: State coordinates [Batch, spatial_dim]
            t: Time values [Batch, 1]

        Returns:
            Diffusion matrix Î£ [Batch, spatial_dim, spatial_dim]
        """
        batch_size = x.shape[0]
        # Expand constant diffusion to batch size
        return self.sigma.unsqueeze(0).expand(batch_size, -1, -1)

    def is_constant_coefficients(self) -> bool:
        """
        Check if drift and diffusion are constant.

        Returns:
            True (Merton model has constant coefficients)
        """
        return True

    def initial_score(self, x: torch.Tensor, t_epsilon: float = 0.1) -> torch.Tensor:
        """
        Initial score function for n-D Merton model.

        For the Dirac delta initial condition pâ‚€(x) = Î´(x - xâ‚€), we evaluate
        the score at a small time t_Îµ to avoid the singularity at t=0.

        At small time, the distribution is approximately Gaussian with mean xâ‚€
        and covariance DÂ·t_Îµ, giving:
            sâ‚€(x) â‰ˆ s(x, t_Îµ) = -(DÂ·t_Îµ)â»Â¹(x - xâ‚€ - Î¼Â·t_Îµ)

        Args:
            x: Spatial coordinates [Batch, spatial_dim]
            t_epsilon: Regularization time for Dirac delta (default: 0.1)

        Returns:
            Initial score sâ‚€(x) [Batch, spatial_dim]
        """
        # Ensure proper shape
        if x.dim() == 1:
            x = x.unsqueeze(0)

        batch_size = x.shape[0]

        # Evaluate at small time to regularize Dirac delta
        t_small = torch.full((batch_size, 1), t_epsilon, device=self.device, dtype=self.dtype)

        # Use analytical score at small time
        return self.analytical_score(x, t_small)

    def analytical_score(
        self,
        x: torch.Tensor,
        t: torch.Tensor
    ) -> torch.Tensor:
        """
        Analytical score function for n-D Merton model.

        For the Gaussian density with mean Î¼_t = xâ‚€ + Î¼t and covariance Dt,
        the score function is:
            s(x, t) = âˆ‡_x log p(x, t) = -(Dt)â»Â¹(x - xâ‚€ - Î¼t)

        Args:
            x: Spatial coordinates [Batch, spatial_dim]
            t: Time values [Batch, 1] or [Batch]

        Returns:
            Score vectors [Batch, spatial_dim]
        """
        # Ensure proper shapes
        if x.dim() == 1:
            x = x.unsqueeze(0)
        if t.dim() == 1:
            t = t.unsqueeze(-1)

        # Avoid t=0
        t_clamped = torch.clamp(t, min=1e-6)

        batch_size = x.shape[0]

        # Time-evolved mean and covariance (per-sample)
        mu_t = self.x0 + self.mu * t_clamped  # [Batch, spatial_dim]

        # Compute per-sample covariance: cov_t[b] = D * t[b]
        # Reshape t for proper broadcasting: [Batch, 1] -> [Batch, 1, 1]
        t_expanded = t_clamped.view(batch_size, 1, 1)  # [Batch, 1, 1]
        # Expand D to batch dimension: [spatial_dim, spatial_dim] -> [Batch, spatial_dim, spatial_dim]
        cov_t = self.D.unsqueeze(0) * t_expanded  # [Batch, spatial_dim, spatial_dim]

        # Compute inverse covariance (batched)
        inv_cov = torch.linalg.inv(cov_t)  # [Batch, spatial_dim, spatial_dim]

        # Score: s(x, t) = -Î£â»Â¹(x - Î¼_t) (batched)
        diff = x - mu_t  # [Batch, spatial_dim]
        score = -torch.einsum('bij,bj->bi', inv_cov, diff)  # [Batch, spatial_dim]

        return score

    def __repr__(self) -> str:
        """String representation."""
        return (
            f"FokkerPlanckMertonND(spatial_dim={self.spatial_dim}, "
            f"Î¼={self.mu.cpu().numpy()}, D_shape={self.D.shape})"
        )
